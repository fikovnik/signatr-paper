\documentclass[sigplan,anonymous,review]{acmart}

\usepackage{authorcomments}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xspace}
\usepackage{paralist}

\input{misc/listing}
\input{misc/macros}
\input{code-and-figures/experiment-db.tex}
\input{code-and-figures/experiment-uf.tex}
\input{code-and-figures/experiment-tracer.tex}

\begin{document}

\title{\tool: A Data-Driven Fuzzing Tool for R}

\begin{abstract}

Dynamic program analysis is often the only way to precisely analyse code written in dynamic programming languages as the semantics of these languages render static analyses unsound. % renders static analysis loose its imprecision to the point is becomes useless.
Dynamic analysis, however, requires both code to run and valid inputs for said code; while one could exclusively analyze existing code (e.g., tests or examples), such code paints an incomplete picture as the permissive semantics of dynamic languages can lead to puzzling runtime behavior.
To obtain more runnable code, one could make use of a \textit{fuzzer}, a tool to generate random inputs for functions, but fuzzing in the context of dynamic languages is tricky, as the shape of user-defined data is difficult to automatically determine ahead-of-time.

In this work, we propose a novel feedback-driven blackbox fuzzing approach which draws function inputs from a database populated with values recorded from existing code.
Unlike traditional use of fuzzing, \Ie, to discover bugs and performance pathologies, our aim to find new \textit{successful} function invocations that can extend the scope of a program analysis.
We implement this approach in a tool called \tool for the R programming language.
We present the insights of its design and implementation, and report on an application in the context of language design.
Concretely, we stress-test one of the proposed type system designs for R, where function types were inferred from recorded call signatures from functions tests and examples.
Fuzzing \UFNumFunctions of those R functions, \tool generated \UFSignatrAdditionalSignatures (\UFSignatrAdditionalSignaturesRatio times) more unique call signatures in comparison to the original study.

\end{abstract}

\maketitle

\section{Introduction}
\label{sec:introduction}

% Fuzzing is a thing, and it is useful.
Fuzzing has established itself as useful technique for test generation and security.
At a high level, fuzzers are automated testing tools that call programs with randomly generated inputs in an effort to uncover bugs, locate security vulnerabilities, generate new software tests, etc.

% Dynamic language semantics make it difficult to fuzz in them.
While fuzzing has seen widespread adoption across many different programming languages, \textit{dynamically typed} programming languages such as Python, JavaScript, and R pose unique challenges revolving around the idea that dynamically typed code tends not to result in runtime errors.
Essentially, the semantics of these languages is very permissive: for instance, accessing a non-existent field of an object in JavaScript yields the value {\tt undefined} instead of crashing, and basic functions in R will readily coerce values whose types do not match (and the coercion rules depend on the function in question).
This is in addition to the lack of static types, which leaves fuzzers with very little information about what types of values are expected.

% Value types are hard to come by.
Another important challenge facing general-purpose fuzzing in the context of dynamic languages is the difficulty in obtaining complex values automatically.
In a statically typed language like Java, the shapes of user-defined objects can be inferred from a static class definition, and this not always possible in, e.g., dynamic prototype-based inheritance in JavaScript, or dynamic object definition in R.
% Thus, an effective fuzzer would need to be manually equipped with the ability to create values of these complex types, which places an undue burden on developers.

% In this paper, ...
To get around this, we propose a general-purpose approach for fuzzing that relies on a database of values observed during the execution of real code.
We develop a tracer that collects information about function calls and values observed during code execution, and this information is stored in a database with an expressive query API.
We propose a fuzzing approach that relies on this database to generate new function inputs based on massive amounts of existing observed values.
This approach is implemented in a tool called \tool for the R programming language.

% Retrofitted.
To help show that the approach discovers new and interesting calls to functions, we built a post-processor that infers types for all of the successful calls \tool uncovered.
As R lacks static types, we use the {\tt contractr} type inference tool from the Turcotte et al. paper proposing a type annotation language for R~\cite{turcotte2020designing}.
This assessment reveals many successful function calls with type signatures \AT{words} 

% As a use case for our tool, we argue that a general-purpose fuzzer can aid data-driven retrofitted type system designers by providing them with unexpected but valid inputs for existing functions.
% We develop an approach for synthesizing the results of fuzzing into a type signature for a function, entirely parameterized over a type system expressed as (1) a function to infer the type of a value, and (2) a function to determine if one type is a subtype of another.


In summary, the contributions of this paper are:
%
\begin{inparaenum}[(1)]
    \item a novel fuzzing technique relying on a database of observed values;
    \item an implementation of this approach in a tool called \tool for the R programming language; and
    \item a use case for \tool where it is used to generate many yet unseen, successful inputs for thousands of R functions.
\end{inparaenum} 

% Using the tool, we generate a value database \TODO{DB stats} and use it to fuzz trace types for \TODO{experiment stats}.
% We find \TODO{result stats}.
% %
The tool is open source and is available online alongside the data set used for this paper.

\section{Background and Related Work}
\label{sec:background}

\paragraph{Related Work}

There are myriad fuzzing tools and approaches, e.g., \emph{Randoop}~\cite{pacheco2007randoop} is a feedback-driven random test generation tool for Java, though the technique underpinning it is universally applicable.
Essentially, sequences of method calls are generated to test classes, and randomly generated arguments for these calls are used in two ways: for primitives, a random value is selected from a predefined, but user-extensible list, and for reference types a value is selected at random from those which have been seen, and if none are available then {\tt null} is selected.
While this technique is effective at generating tests involving non-trivial objects that are built up from a number of method calls, these objects are uncommon in data science languages, where the challenge is generating realistic data.
\emph{QuickCheck}~\cite{quickcheck} is a tool for Haskell that allows programmers to express program properties that are subject to random fuzz testing.
American Fuzzy Lop (AFL)~\cite{afl} is a state-of-the-art industrial fuzzer; AFL takes a program and one example file as input, calls the program with the input, and then uses a variety of heuristics to transform the input and fuzz the program.
\AT{Do we want to say:} In this work, we propose an approach to fuzzing where no input is required from the programmer, and automatically discovers how the function or program under study should be called.

\paragraph{The R Programming Language}

We will present a fuzzing tool called \tool for the R programming language.
R sports an unusual mix of language features making it a challenging target for
tooling~\cite{morandat2012evaluating}. 
Some major challenges include:

\begin{compactitem}[$-$]

\item R does not have type annotations or a static type system, so there is very little to suggest what are expected arguments or return values.
% This means there is nothing to suggest what the expected arguments or return values of a function could be, and thus there is little to guide test generation.

\item Primitive values (booleans, integers, doubles, complex numbers, and character strings) each have a separate ``NA'' value indicating that data is ``not available''--some operations fail on these NAs.

\item Most values are vectors or lists, and can be annotated by key-value pairs called \textit{attributes}. 
These annotations, coupled with reflection, are the basic building blocks for many advanced features of R. 
For example, the S3 dynamic dispatch mechanism is based on the class attribute of a value.
(In fact, R has multiple different object systems but S3 is the most widespread one.)
An R object is simply a value with \code{class} attribute denoting its classes as a vector of character strings (values often have multiple classes).

\item Built-in types are automatically and silently coerced from more specific types to more general types, and each function coerces its parameters as it sees fit.

% I don't think we need promises.
% \item All expressions are evaluated by-need, thus the call \code{f(a+b)}
%     contains three delayed sub-expressions, one for each variable and one for
%     the call to plus. This means that R does not pass values to functions but
%     rather passes unevaluated promises (the order of evaluation of promises is
%     part of the semantics as they can have side effects). These promises can
%     also be turned back into code by reflection.

% \item R has a copy-on-write semantics for shared values. A value is shared if
%     it is accessible from more than one variable. This means that side effects
%     that change shared values are rare. This gives a functional flavor to large
%     parts of R.

% \item The primary abstraction in R is a function. Functions are grouped into
%     packages that can be imported. A function can be generic, allowing an
%     object-oriented style of programming. \TODO{Some stats about how many
%     parameters R functions have.} 

\end{compactitem}



% R ecosystem stuff.
The primary abstraction in R is a function, and functions are grouped into packages that can be imported. 
A function can be generic, allowing an object-oriented style of programming. \TODO{Some stats about how many parameters R functions have.}

% Types for R.
There has been some effort to develop a static type system for R.
Turcotte et al.~\cite{turcotte2020designing} proposed a simple type annotation language for types at the level of functions, inspiring themselves from a corpus analysis of 400 R packages.
As part of our assessment of \tool, we will run it on the functions in this corpus to demonstrate the many novel and unexpected ways functions can be called.


% \subsection{Test Generation for Dynamic Languages}

% Many of the aforementioned techniques rely on static function parameter types in creating values with which to call a function, and dynamic languages do not have static type information.
% For example, \emph{LambdaTester}~\cite{lambdatester} focuses on test generation for higher-order functions in JavaScript; a discovery phase is required to identify which parameters are expected to be callbacks, and all other, non-callback arguments are generated in a similar manner to \emph{Randoop}.
% Further work on \emph{Nessie}~\cite{arteca2022nessie} expanded upon the approach presented in \emph{LambdaTester} to generate tests for asynchronous callbacks using sequencing and nesting.
% Other work on fuzzing deep-learning libraries in Python~\cite{wei2022free} explicitly cite Python being a dynamic language as a challenge for test generation; an important part of the pipeline in the paper is inferring types for function parameters by running existing code.

\section{Approach}
\label{sec:fuzzy}

In this section, we describe the design and implementation of the \tool fuzzer.
At the end, in the Appendix~\ref{sec:demo}, we show an example how to use it.

\subsection{Overview}

The tool provides two main operations:
%
\begin{inparaenum}[(1)]
\item \emph{recording}, where it runs R code and records each function invocation into the value database (\sxpdb), and
\item \emph{fuzzing}, where it uses the recorded values to generate input into R functions. 
\end{inparaenum}
%
The tool is meant to be run as a part of a data pipeline that analyzes a corpus of code.
This is pictured in the diagrams in Figures~\ref{fig:sxpdb-pipeline} and \ref{fig:fuzz-pipeline} for the recording and fuzzing modes respectively.

First, we extract the runnable code from R packages (snippets of examples from documentation and tests) into R scripts.
Next, in parallel, we run the scripts, recording all unique function parameters into \sxpdb.
Finally, we merge all the \sxpdb into one while discarding duplicates.
We use R OpenSci targets~\cite{landau2021_targets} for orchestration, but any generic batch scheduler will do as well.

Once we have the value database, we can start fuzzing.
The fuzzer is parameterized by the concrete dynamic analysis we are interested in.
There are two main hooks: before a function is called allowing one to preprocess the input, and after the function is done executing allowing one to process the output or error depending whether the call was successful or not.
The after-invocation hook can also provide feedback to the fuzzer, currently we support signalling success or failure.
In dynamic analysis, we are often interested in much finer information about the function execution than simply start and end.
For example, for the call signature inference use case, we would like to know if any of the function arguments was coerced to another type or if there was a method dispatch on it.
This is supported via instrumentation.
The fuzzer runs the functions using \emph{R-dyntrace}, an extended R virtual machine that supports attaching callbacks to various runtime events~\cite{goel2019}.
The dynamic analysis can therefore use one of the 40 runtime events to get the necessary insights (including dispatch and coercion).

A natural way is to run one instance of fuzzer per target function.
Each instance consists of two processes.
The reason is that R could crash (and it does, \Cf Section~\ref{sec:assessment}) and we do not want to loose the partial results.
Therefore we have one worker doing the fuzzing and another supervising re-spawning the worker when it dies.
Furthermore, we run the fuzzing isolated in a container as "random" values goes into real code which could have dare consequences.

The tool consists of three standalone components: \code{argtracer} the tracer responsible for running the R code and recoding function invocation, \code{sxpdb} the value database, and \code{generatr} the fuzzer responsible for generating inputs.
Technically, they are R packages written in combination of R and C++.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{code-and-figures/sxdb-pipeline.pdf}
    \caption{
    Recording pipeline.
    % Runnable code snippets are extracted from an input list of R packages.
    % These snippets are fed in parallel to the {\tt sxpdb-create} part of \tool, which picks out values and metadata related to them (e.g., the origin of the value). 
    % This creates a number of disparate databases that are merged into one database, which is referred to as the {\tt sxpdb}.
    }\label{fig:sxpdb-pipeline}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{code-and-figures/fuzz-pipeline.pdf}
    \caption{
    Fuzzing pipeline.
    % The {\tt sxpdb}, along with a list of R functions to fuzz, is input to the fuzzing phase of \tool.
    % The fuzzing phase fuzzes each of the functions w.r.t. the {\tt sxpdb}, which results in a collection of execution traces; these execution traces are fed to the typing phase of \tool, which infers types for arguments and outputs a list of unique call signatures.
    }\label{fig:fuzz-pipeline}
\end{figure}

\subsection{Tracer}\label{sec:argtracer}

The tracer is built on top of \emph{R-dyntrace}.
The two main runtime events we use are a function exit and a context jump.
The former one is where we capture all the evaluated arguments and pass them to \sxpdb.
The latter one is necessary to keep our call stack in balance.
The R interpreter uses long jumps for loops control flow, return statements and error handling.
In the case of long jump, the exit hook is ignored, to make sure, we do not loose any function exit, we maintain our version of R call stack.

When we see a call, we only get a pointer to R closure.
The function name and the package that contains it can only be found by looking at the currently loaded namespaces and the symbols they contain.
Because of this, we eagerly build an index of package and function names when we see a call to load namespace function (package loading in implemented in R).
The fact that R is implemented in a mixture of R and C requires that we maintain a black list of R calls that are connected to R internals.
Overall, we ignore 70 functions that are used for namespace loading and manipulation, and reflection.

Regarding performance, there is \TRMinTracingOverhead --- \TRMaxTracingOverhead (\TRAvgTracingOverhead on average) slowdown when running the R code with tracing enabled (based on the running / tracing \TRTracingFiles R scripts recording 8.2M unique values of 3.2GB size).
The main cost comes from value serialization (\Cf Section~\ref{sec:sxpdb}).
The tracer is written in 600 lines of C++ code.

\subsection{Database of Values}\label{sec:sxpdb}

To store recorded R values we developed a our own database.
The reason was two-fold: (1) we can leverage some domain-knowledge of R values and store them efficiently, (2) we can optimize it for the type of queries that the fuzzer does.
It is implemented in C++ with R API (in 4.5K lines of C++ code and 1.5K lines of R code).

\paragraph{Storage}

The database stores only unique values.
We use XXH128, a 128bit hashing algorithm\footnote{fast non-cryptographic hash algorithm, \Cf \url{https://cyan4973.github.io/xxHash/}} for uniqueness.
While the hashing is fast, it works on an array of bytes so we need to lower R values into a binary format.
R provides a binary serialization format XDR\footnote{\Cf \url{https://cran.r-project.org/doc/manuals/r-release/R-ints.html\#Serialization-Formats}}, but it is costly.
Since there is a large number of values being pushed to the database in an average recording session, we try to avoid the serialization as much as possible.
For this we use the \emph{trace} bit that is part of the R value header\footnote{A bit in the \code{SEXP} C struct that represents R value. \Cf \url{https://cran.r-project.org/doc/manuals/r-release/R-ints.html\#Rest-of-header}}.
If it is not set, then it is a fresh value so we serialize it, compute its hash, store it into the database and set the trace bit.
If it is set, we have seen the value before, however, despite that R implements a copy-on-write semantics, until a value is aliased, it can be modified in place.
Therefore, we need to check whether is has been shared and if it has not, we have to serialize it and check its presence using the hash.

\FK{Some info about how the data layout?}

Next to the value itself, the database also store metadata about the value and its origin (the package, function and argument name).
We also store a unique call ids for each sequence of argument values coming from the same call.
This way we can reproduce the calls that were recorded by the tracer.

Currently, the database supports all R values except for environments, closures and external pointers.

\paragraph{Queries}

% Can relax on: "na", "length", "attributes", "type", "vector", "ndims", "class"
The database provides a convenient query API based on this information, which represents search parameters for the database and is comprised of the {\tt typeof}-type, the class, the presence of NAs, the attributes, if the value is a vector or scalar, the dimensions (for tabular data), and length of the value (for list data).
% Importantly, the database provides a convenient query API: i.e., one may sample values randomly from the database according to some parameters, such as the type \PB{Should we be more precise?}, or by providing an example value on which some characteristics (again, such as the type) can be relaxed.
The database can be queried for a random value with the desired metadata (e.g., a vector of integers of length 10), or can be queried by providing an existing value along with the search parameters to be relaxed (e.g., query the database for a value similar to {\tt 42}, but relax on the length).

\FK{More info, but I'm not familiar with this}

\subsection{Fuzzing Technique}

The database of values is at the core of the fuzzing approach, which is similar in spirit to mutation-based fuzzing (where instead of mutating arguments to previous calls, new argument values are selected based on previous ones).

The core of fuzzing is to generate new calls to a function $f$, and the process of choosing arguments to construct these calls is depicted in Algorithm~\ref{alg:arg-sel}.
In addition to the function $f$ and value database $db$, the algorithm considers how many query parameters to relax on ($numRelax$), as well as all of the previously seen successful calls to $f$ ($succs$).
For each parameter $p$ of $f$, the algorithm determines which parameters to relax on (this may change from one iteration to the next), finds all values that inhabited $p$ in successful calls to $f$, chooses one such value $v$, and queries the database for a value similar to $v$ in all respects, save for the parameters that are being relaxed this time.
Note: if no successful calls to the function have been observed, random values for parameters can be chosen.

The fuzzing approach itself is depicted in Algorithm~\ref{alg:fuzzer}.
First, the collection of already known successful calls to $f$ is obtained from the database $db$.
The main idea of the approach is to start by selecting new arguments essentially at random by querying the database and relaxing on many parameters, and gradually reduce the number of parameters being relaxed as the fuzzer progresses.
Concretely, the number of parameters being relaxed is reduced every $tick$, which is determined by dividing the total fuzzing budget by the number of parameters that can be relaxed ($numRelaxParameters$).
The function $f$ will be fuzzed for as long as the budget allows, and initially all database parameters will be relaxed.
Arguments for a new call to $f$ are generated through the approach depicted in Algorithm~\ref{alg:arg-sel} ($getNewArgs$), the call is performed, and the results are saved in $res$.
If there were no errors, warnings, or crashes, then the successful call is added to the list of successful calls $succs$, and iteration continues until the budget is exhausted.

\input{code-and-figures/approach-algorithms.tex}

\subsection{Use Case: Typing Successful Traces}

To help show that this fuzzing approach uncovers interesting calls, we inferred types from the resulting successful calls $succs$.
As R has no static type system, we use the type system proposed by Turcotte et al.~\cite{turcotte2020designing} by utilizing their {\tt contractr} type inference tool.
% This is the stage where a user can define their type system, which is input as two functions: one to infer the type of some value $v$, and another to take two types $t_1$ and $t_2$ and judge whether or not they can be simplified (e.g., determine if $t_1$ is a subtype of $t_2$, $t_1 <: t_2$).
This helps determine the set of \textit{unique} call signatures for a function, as two calls with different values that have the same type can be collapsed at this stage.
For example, say the identity function {\tt id} was called with {\tt 4} and {\tt 2}; the types of each of those calls is $int \rightarrow int$, and so they would be collapsed together. 

The fuzzing approach is implemented in a tool called \tool, written in R and C++, and is available as an R package.

\section{Assessment}
\label{sec:assessment}

The main motivation for building \tool was to have a tool that can generate successful calls to R functions, \emph{in addition} to what we can record by running the available executable code from R packages.
Therefore, to assess the usefulness of \tool we need to answer \emph{how many additional successful calls can \tool generates?}.

We ran all of our experiments on Intel Xeon 6140, 2.30GHz with 72 cores and 256GB of RAM running Ubuntu 18.04.

We base the experiment on the same corpus as we used in the \typer paper (412 packages, 17.4K public functions).
First, we created a \sxpdb for the fuzzer.
For this we used the extracted runnable code from the corpus packages which consists of \DBNumSourceFiles files containing \DBSourceLinesOfCodeRnd lines of R code (excluding comments and new lines).
Generating the database took \TODO{XXX - @pierre} and takes \DBFileSize of disk space.
It contains \DBValuesRnd unique values recorded from \DBNumCallsRnd calls to \DBNumFunctionsRnd functions in \DBNumPackages packages.
\FK{Info about value distribution? A pie chart?}

With this database, we fuzzed \UFNumFunctions functions from \UFNumPackages packages, a subset of the original corpus.
Each function was run \UFTracingBudget times, taking it 36 hours while fuzzing 32 functions in parallel\footnote{Trying bigger number resulted in tasks being killed due to running out of memory.}.

In total we made \UFNumTracesRnd calls (\UFTracesSize of traces and \UFTracesReturnDbsSize). Out of that, \UFRatioSuccessTraces were successful, resulting in \UFNumSuccessTraces traces of \UFNumSuccessFunctions functions coming from \UFNumSuccessPackages packages.
The vast majority of errors were exceptions, but in \UFNumOfCrashedRSessions cases, R processed crashed.

\FK{Report the stringi bug that was fixed}




% how often R crashed

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{code-and-figures/uf-call-signatures.pdf}
    \caption{
        Number of call signatures obtained by fuzzing in addition to tracing. Each line represents one function.
        \FK{an alternative would be to show proportionally how many out of all sigantures come from fuzzing / traing - no need for log scale then}
    }\label{fig:call-signatures}
\end{figure}

\section{Conclusions and Future Work}
\label{sec:conclusions}

Fuzz testing is an incredibly useful technique for finding bugs and security vulnerabilities in code, but is hindered by the permissive semantics of dynamic languages as well as the dynamic nature of how complex data is defined.
In this work, we proposed a fuzzing approach that relies on a database of observed values to provide complex and realistic inputs for functions.
We implement this approach in a tool called \tool for the R programming language, and show that \tool uncovers many new and interesting call signatures for R functions.

While this approach was implemented in a tool for R, it is broadly applicable in all languages, and the only real language-specific aspect is the set of parameters in the database.
For instance, one could implement a similar tool for object-oriented languages, where database metadata could include object field names (e.g., JavaScript).

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{fuzzing}

\appendix

\section{\tool demonstration}\label{sec:demo}

The following is a short demo of the basic \tool functionality, \Ie how to create the value database by running R code and then use it to fuzz function type traces.
We will be using the command line interface but the same is available directly in R and thus directly usable from pipeline frameworks such as targets.

\FK{Once I have it up and running I will split the listings into multiple, fix the new lines and add the description to each of the command.}

\begin{lstlisting}
$ cat example-1.R
...

$ cat example-2.R
...

$ ls example*.R | parallel signatr record --from '{1}' --db '{1.-sxpdb}'
...

$ ls -l
...

$ signatr sxpdb-info example-1-sxpdb
...

$ signatr sxpdb-info example-2-sxpdb
...

$ signatr sxpdb-merge --output merged-sxpdb example-*-sxpdb
...

$ signatr sxpdb-info merged-sxpdb
...

$ signatr fuzz --target "base:::`+`" --db merged-sxpdb --budget 100 --output traces
...

$ ls -lh fuzz
...

$ signatr type --traces traces --output types.csv
...

$ wc -l types.csv
...

$ head -10 types.csv
...

$ R -e 'read.csv("type.csv")["signature"] |> unique |> length'
...

\end{lstlisting}

\end{document}
\endinput
